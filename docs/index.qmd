---
title: Predicting Heart Disease from Cleveland Database
jupyter: python3
author: "Albert C. Halim, Archer Liu, Stephanie Wu, & Ziyuan Zhao"
date: "November 29, 2024"
format:
  html:
    toc: true
    toc-depth: 2
  pdf:
    toc: true
    toc-depth: 2
toc-title: Table of Contents
number-sections: true
bibliography: references.bib
link-citations: true
execute:
  echo: false
  warning: false
editor: source
---

```{python}
import pandas as pd
from IPython.display import Markdown, display
from tabulate import tabulate
```

```{python}
coefficient_df = pd.read_csv("../results/tables/coefficient_df.csv", index_col=0)
best_model_cv_results_df = pd.read_csv("../results/tables/best_model_cv_results.csv", index_col=0)
misclassified_examples_df = pd.read_csv("../results/tables/misclassified_examples.csv")
baseline_cv_results_df = pd.read_csv("../results/tables/baseline_cv_results.csv", index_col=0)
test_scores_df = pd.read_csv("../results/tables/test_score.csv").round(2)

svc_test_score = float(test_scores_df.iloc[0]['Accuracy'])
lr_test_score = float(test_scores_df.iloc[1]['Accuracy'])
```

# Summary

In this project, we developed and evaluated several classification models to predict the presence of heart disease using the Cleveland Heart Disease dataset [@Detrano1988], which includes various clinical features. We compared four models: Logistic Regression, Support Vector Classifier (SVC), Dummy Classifier (as a baseline), and Decision Tree Classifier. Logistic Regression performed the best, achieving high accuracy of `{python} lr_test_score` and providing interpretable coefficients that helped us understand the impact of each feature on heart disease prediction. The SVC also performed well but slightly lagged behind Logistic Regression in test accuracy with `{python} svc_test_score`. The Dummy Classifier served as a baseline, emphasizing the need for more sophisticated models, while the Decision Tree Classifier showed reasonable performance but tended to overfit. Misclassifications were analyzed to identify potential feature engineering opportunities, and future work could include exploring alternative classifiers such as Random Forests. Additionally, incorporating probability estimates into predictions would enhance the model's clinical usability, providing clinicians with more confidence in the results.

# Introduction

According to a 2022 CDC federal report, the leading cause of death in the United States is heart disease [@CDC2022]. At 702,880 deaths that year, the disease claimed the most lives out of popular longevity doctor Peter Attia's so-called four horsemen, and nearly as many as cancer and diabetes (two of his other horsemen) combined [@Attia2023]. Despite past efforts having led to promising declines in mortality rates, the disease has gained traction within the last 5 years in particular[@Bui2024]. As such, early detection of heart disease, not to mention increased understanding of and heightened mindfulness around mitigating the risk factors for heart disease, can help improve countless lives in the United States and elsewhere.

Here we ask if we can use a machine learning algorithm to predict whether an individual has the presence of heart disease given a relevant selection of their bioinformatic data. Answering this question is important because both patients and their health teams can seek to benefit from tooling and technologies that help in the diagnostic process of such a prevalent disease. Given such prevalence, not to mention the potential gravity of heart-related conditions, detecting heart disease accurately and early on with a scalable solution in medical settings can help enhance medical care in terms of both timeliness and preparedness, to name a few aspects. Thus, if a machine learning algorithm can accurately and effectively predict whether an individual may harbour this disease early on, this could advance the timeline of early intervention, scale heart disease diagnosis efforts, and lead to better patient outcomes, as well as reduce the risk of future complications implicated with having heart disease.

# Methods

## Data

For this project, we will be using the Heart Disease UCI dataset created by R. Detrano, A. JÃ¡nosi, W. Steinbrunn, M. Pfisterer, J. Schmid, S. Sandhu, K. Guppy, S. Lee, and V. Froelicher at the Department of Medicine, Veterans Administration Medical Center, Long Beach, California [@Detrano1988]. It was sourced from the UC Irvine Machine Learning Repository [@Detrano1988] and can be found [here](https://archive.ics.uci.edu/dataset/45/heart+disease). The specific file used represents the Cleveland locality. The dataset contains 303 rows, with each row representing summary statistics for a particular patient, and 14 columns with 13 features and 1 target variable. The target variable is the diagnosis of heart disease (angiographic disease status), and the value 0 is for no diagnosis of heart disease and the value 1 is for the diagnosis of heart disease. The 13 features are as follows:

- Age
- Sex
- Chest pain type
- Resting blood pressure
- Serum cholesterol
- Fasting blood sugar
- Resting electrocardiographic
- Maximum heart rate achieved
- Exercise induced angina
- Oldpeak = ST depression induced by exercise relative to rest
- The slope of the peak exercise ST segment
- Number of major vessels
- Thalassemia blood disorder

They are encoded in the dataset as follows:

- \#3 (age)
- \#4 (sex)
- \#9 (cp)
- \#10 (trestbps)
- \#12 (chol)
- \#16 (fbs)
- \#19 (restecg)
- \#32 (thalach)
- \#38 (exang)
- \#40 (oldpeak)
- \#41 (slope)
- \#44 (ca)
- \#51 (thal)
- \#58 (num) (the predicted attribute)

## Preprocessing

The preprocessing steps are crucial for preparing the heart disease dataset for analysis and model training. This section outlines the steps taken to clean and transform the raw data before it is split into training and test datasets.

Firstly, directories are created to store raw and processed data. If the directories do not already exist, they are created programmatically. 

Next, the heart disease dataset is downloaded as a ZIP file from the UCI Machine Learning Repository. Once downloaded, the ZIP file is extracted to the raw directory.

After extraction, the dataset is read into a Pandas DataFrame. The column names are defined manually, as the dataset does not include a header row. At this point, it was noted that the raw dataset uses the string '?' to represent missing values. These values are replaced with NaN to facilitate proper handling during further analysis.

As briefly mentioned in the above section, the target variable, `num`, represents the presence or absence of heart disease. The original values of num range from 0 to 4, where any value greater than 1 indicates the presence of heart disease. To simplify the analysis, all values greater than 1 are mapped to 1 (i.e., indicating the presence of heart disease).

Finally, to prepare the dataset for model training and evaluation, it is split into training and test subsets. The split is done in a 70/30 ratio, with stratification to ensure that the distribution of the target variable num is preserved in both subsets. The resulting training and test sets are then saved as CSV files for future use such as in the next steps, which will involve analyzing the data, training models, and assessing their performance.

## Analysis

In this project, we used the Logistic Regression, SVC, Decision Tree, and Dummy Classifier as a baseline to build a classification model aimed at predicting the presence of heart disease based on clinical features. We used all available features from the dataset, excluding some variables related to the error of certain measurements. The data was split into a training set (70%) and a test set (30%). To choose the best value for the hyperparameter k, we used 5-fold cross-validation, with accuracy as the classification metric. We also standardized the data before fitting the model to ensure the features were on a similar scale. The analysis was carried out using Python, with the following libraries: NumPy, Pandas, scikit-learn, and Matplotlib.

# Results & Discussion

```{=html}
<div style="background-color:#cce7ff; padding:15px; border-radius:5px; border: 2px solid #007acc;">
    <h2 style="color:#007acc;">ðŸ“Š Visualization Section</h2>
    <p style="font-size:14px; color:#005f99;">
        The following plots provide insights into the dataset, including target variable distribution, categorical feature relationships, 
        and model performance (confusion matrix). Each visualization highlights critical aspects of the analysis.
    </p>
</div>
```

### Data Set Summary

The heart disease dataset used in this project is obtained from the UC Irvine Machine Learning Repository. The dataset contains 13 features, and the target is a binary variable (`num`) where:

- **`0`**: No presence of heart disease
- **`1` or higher**: Presence of heart disease.

Out of the 13 features:
- **8 are categorical** (e.g., `sex`, `cp`, `thal`).
- **5 are numeric** (e.g., `age`, `chol`, `thalach`).

These features include various physiological parameters, such as:
- **Resting blood pressure**,
- **Serum cholesterol levels**,
- **Maximum heart rate achieved**.

Additionally, it records potential signs of heart disease, such as chest pain type (`cp`) and exercise-induced angina (`exang`).

The dataset contains **303 observations**, and the original study used a Bayesian model to estimate the probability of having heart disease [@Detrano1988].

As shown in @fig-numeric-distributions, the numeric features of the dataset have varying distributions, which provide an overview of the data's spread and tendencies.

![Distribution of numeric features in the dataset.](../results/figures/raw_feature_distributions.png){#fig-numeric-distributions width=70%}  

The target variable (`num`) is distributed as displayed in @fig-target-distribution, where the majority of the samples are classified into the lower categories of heart disease.

![Distribution of the target variable (`num`).](../results/figures/target_variable_distribution.png){#fig-target-distribution width=75%}  

For categorical features, the grouped distributions based on the presence of heart disease are shown in @fig-categorical-distributions. These stacked bar plots illustrate the relationship between categorical variables and the target variable.

![Distribution of categorical features grouped by heart disease (`num`).](../results/figures/categorical_features_distribution.png){#fig-categorical-distributions width=70%}  

Boxplots for selected numeric features grouped by the target variable (num) are presented in @fig-boxplots-class, offering insights into the spread and central tendencies of the selected features for each heart disease category.

![Boxplots of selected numeric features grouped by heart disease (`num`).](../results/figures/raw_boxplots_by_class.png){#fig-boxplots-class width=75%}  


```{=html}
<div style="background-color:#fce8e6; padding:15px; border-radius:5px; border: 2px solid #ff6666;">
    <h2 style="color:#ff6666;">âœ… Visualization Section Completed</h2>
    <p style="font-size:14px; color:#cc0000;">
        This concludes the visualization section. The insights derived from these plots will be used to guide subsequent analyses 
        and modeling steps. If additional visualizations are required, they can be added here.
    </p>
</div>
```

## Discussion

```{python}
#| label: tbl-baseline_cv_results_df
#| tbl-cap: Baseline Model CV Results

Markdown(baseline_cv_results_df.to_markdown())
```

```{python}
#| label: tbl-best_model_cv_results_df
#| tbl-cap: Best Model CV Results

Markdown(best_model_cv_results_df.to_markdown())
```

```{python}
# Remove the ' (+/- ...)' part and convert to float
best_model_cv_results_df['test_score'] = best_model_cv_results_df['test_score'].str.extract(r'([0-9.]+)').astype(float)
best_model_cv_results_df['train_score'] = best_model_cv_results_df['train_score'].str.extract(r'([0-9.]+)').astype(float)

# Extracting values for Logistic Regression and SVC from index 0
logreg_test_score = best_model_cv_results_df.iloc[2]['test_score']
logreg_train_score = best_model_cv_results_df.iloc[2]['train_score']
svc_test_score = best_model_cv_results_df.iloc[1]['test_score']
svc_train_score = best_model_cv_results_df.iloc[1]['train_score']

# Calculating the gaps
logreg_gap = float(round(logreg_train_score - logreg_test_score, 3))
svc_gap = float(round(svc_train_score - svc_test_score, 3))
```

After doing checking on the baseline (@tbl-baseline_cv_results_df) we proceed to do the cross-validation. In the final cross-validation results (@tbl-best_model_cv_results_df), both the best SVC and best Logistic Regression achieve excellent test scores. The small gap between their training and test scores suggests that both models generalize well, with minimal overfitting. However, Logistic Regression has a smaller gap between training and test scores (`{python} logreg_gap`) compared to SVC (`{python} svc_gap`), suggesting that it might generalize slightly better than SVC.


```{python}
#| label: tbl-test_scores_df
#| tbl-cap: Test Score Results

Markdown(test_scores_df.to_markdown())
```

This is further confirmed by the test scores @tbl-test_scores_df, which show that Logistic Regression slightly outperforms SVC on unseen data.

```{python}
#| label: tbl-coefficient_df
#| tbl-cap: Detailed Coefficients for Logistic Regression

Markdown(coefficient_df.to_markdown())
```

To better understand the relationship between each feature and heart disease presence, we examine the coefficients obtained from the logistic regression model (@tbl-coefficient_df). Each coefficient indicates how the corresponding feature influences the likelihood of heart disease. Positive coefficients suggest that as the feature increases, the likelihood of having heart disease increases as well, while negative coefficients suggest the opposite.

![Coefficient Table for Logistic Regression](../results/figures/log_reg_feature_coefficients.png){#fig-logreg-coefficients width=85%}  

In @fig-logreg-coefficients, we can see that features like `ca`, `oldpeak`, and `trestbps` have relatively high positive coefficients, meaning they strongly influence the prediction of heart disease. This makes sense, as research shows that high blood pressure is one of the most important causes of heart disease [@Fuchs2020]. For oldpeak specifically, research shows that ST depression during exercise is linked to higher risk of heart disease[@Carlen2019]. In contrast, features like `thalach` have large negative coefficients, suggesting they are linked to a lower likelihood of heart disease. Features like `age` and `chol`, however, show little impact, as their coefficients are close to zero.

Interestingly, females (`sex` = 0) are more likely to be free of heart disease, as indicated by the large negative coefficient for `onehotencoder__sex_0.0`. In contrast, males (`sex` = 1) are more likely to have heart disease, as reflected by the high positive coefficient for `onehotencoder__sex_1.0`. This is supported by @Regitz-Zagrosek2023, which highlights how biological sex differences, such as premenopausal women having a relative protection from coronary artery disease.

However, there are some limitations of this study. First of all, as categorical features were split into multiple binary columns, interpreting the coefficients for these encoded variables can be tricky. It can be difficult to directly correlate the coefficients with the original feature, and whether this approach is reasonable should also be questioned.

Additionally, while the model's coefficients offer useful insights, they should be taken with caution. Further exploration into feature relationships and more advanced modeling techniques might be required to better understand the complexities of predicting heart disease.

```{python}
#| label: tbl-misclassified_examples_df
#| tbl-cap: Misclassified Examples

Markdown(misclassified_examples_df[:5].to_markdown())
```

```{python}
# Extract the necessary values from the dataframe
false_positive_0_predicted = float(misclassified_examples_df.iloc[0]['Predicted Label'])
false_positive_0_true = float(misclassified_examples_df.iloc[0]['True Label'])
false_positive_0_chol = float(misclassified_examples_df.iloc[0]['chol'])
false_positive_0_oldpeak = float(misclassified_examples_df.iloc[0]['oldpeak'])
false_positive_0_ca = float(misclassified_examples_df.iloc[0]['ca'])

false_negative_1_thalach = float(misclassified_examples_df.iloc[1]['thalach'])
false_negative_2_thalach = float(misclassified_examples_df.iloc[4]['thalach'])
false_negative_1_slope = float(misclassified_examples_df.iloc[1]['slope'])
false_negative_2_slope = float(misclassified_examples_df.iloc[2]['slope'])
```
From @tbl-misclassified_examples_df, we can see that false Positives (e.g., index 0): Predicted as `{python} false_positive_0_predicted` (positive for heart disease), but true label is `{python} false_positive_0_true`.
This individual has a high cholesterol level (`chol` = `{python} false_positive_0_chol`), moderate `oldpeak` (`oldpeak` = `{python} false_positive_0_oldpeak`), and significant `ca` = `{python} false_positive_0_ca`, which might make the model lean toward predicting heart disease incorrectly.

False Negatives (e.g., indices 1, 2, 3, 4): Predicted as `0` (no heart disease), but true label is `1`.
Many of these cases involve features like high `thalach` (e.g., `{python} false_negative_1_thalach`, `{python} false_negative_2_thalach`) and `slope` = `{python} false_negative_1_slope` or `{python} false_negative_2_slope`, which the model might not weigh heavily enough.

Overall the Logistic Regression model performs well and could be useful as a first-pass screening tool in a clinical setting, but there are ways we can make it even better. First, we can take a closer look at the misclassified examples and compare them to correctly classified ones. This could help us identify features or patterns the model struggles with and guide us in improving the features or adding new ones that capture important relationships.

Next, we could test other classifiers to see if they perform better. For example, Random Forests are good at handling feature interactions automatically, which could help improve accuracy.

Finally, instead of just giving a prediction, the model could provide a probability for each class. This would help clinicians understand how confident the model is in its predictions. For low-confidence cases, additional tests or evaluations could be done to avoid mistakes.

These changes could make the model even more accurate and useful in practice.

# References
